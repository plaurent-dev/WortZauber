#!/usr/bin/env python3
"""
Script pour r√©cup√©rer les images de paysages allemands depuis best-wallpaper.net
"""

import requests
from bs4 import BeautifulSoup
import json
import os
from urllib.parse import urljoin
import time

# URL √† scraper
BASE_URL = "https://fr.best-wallpaper.net/Search/q=paysage_allemagne"

# Dossier de destination
IMAGES_DIR = "Background"
os.makedirs(IMAGES_DIR, exist_ok=True)

def get_wallpapers(url):
    """R√©cup√®re les URLs des wallpapers depuis le site"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        wallpapers = []
        
        # Chercher tous les √©l√©ments image dans les r√©sultats de recherche
        # Adapter les s√©lecteurs CSS selon la structure du site
        for img_container in soup.find_all('div', class_='thumb-item'):
            try:
                # Chercher le lien vers la page de l'image
                link = img_container.find('a')
                if link and link.get('href'):
                    wallpapers.append(link['href'])
            except Exception as e:
                print(f"Erreur lors du parsing d'une image: {e}")
                continue
        
        return wallpapers
    
    except requests.exceptions.RequestException as e:
        print(f"Erreur lors de la requ√™te: {e}")
        return []

def get_image_url_1080p(wallpaper_page_url):
    """R√©cup√®re l'URL 1080p d'une page de wallpaper"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(wallpaper_page_url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Chercher le lien de t√©l√©chargement 1080p
        # Adapter selon la structure du site
        download_links = soup.find_all('a', href=True)
        
        for link in download_links:
            href = link.get('href', '')
            # Chercher les URLs contenant "1080"
            if '1080' in href or '1920' in href:
                return href
        
        return None
    
    except Exception as e:
        print(f"Erreur lors de la r√©cup√©ration de {wallpaper_page_url}: {e}")
        return None

def download_image(url, filename):
    """T√©l√©charge une image"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        filepath = os.path.join(IMAGES_DIR, filename)
        with open(filepath, 'wb') as f:
            f.write(response.content)
        
        print(f"‚úÖ T√©l√©charg√©: {filename}")
        return filepath
    
    except Exception as e:
        print(f"‚ùå Erreur t√©l√©chargement {filename}: {e}")
        return None

def save_urls_to_json(urls_dict):
    """Sauvegarde les URLs dans un fichier JSON"""
    output_file = "german_wallpapers_1080.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(urls_dict, f, ensure_ascii=False, indent=2)
    print(f"\n‚úÖ URLs sauvegard√©es dans {output_file}")

def main():
    print("üîç R√©cup√©ration des images de paysages allemands...")
    print(f"Source: {BASE_URL}\n")
    
    # √âtape 1: R√©cup√©rer les pages des wallpapers
    wallpaper_pages = get_wallpapers(BASE_URL)
    print(f"Trouv√© {len(wallpaper_pages)} wallpapers\n")
    
    if not wallpaper_pages:
        print("Aucun wallpaper trouv√©. Le site a peut-√™tre chang√© de structure.")
        return
    
    # √âtape 2: Pour chaque page, r√©cup√©rer l'URL 1080p
    urls_dict = {}
    count = 0
    
    for idx, page_url in enumerate(wallpaper_pages[:20]):  # Limiter √† 20 pour commencer
        print(f"[{idx+1}/{len(wallpaper_pages[:20])}] Traitement: {page_url}")
        
        # Assurer que l'URL est compl√®te
        full_url = urljoin(BASE_URL, page_url) if not page_url.startswith('http') else page_url
        
        # R√©cup√©rer l'URL 1080p
        image_url = get_image_url_1080p(full_url)
        
        if image_url:
            count += 1
            key = f"paysage_allemagne_{count}"
            urls_dict[key] = image_url
            print(f"  ‚úÖ URL trouv√©e: {image_url[:60]}...")
            
            # T√©l√©charger l'image
            filename = f"wallpaper_{count:03d}.jpg"
            download_image(image_url, filename)
        else:
            print(f"  ‚ö†Ô∏è Pas d'URL 1080p trouv√©e")
        
        # Pause pour ne pas surcharger le serveur
        time.sleep(1)
    
    # √âtape 3: Sauvegarder les URLs
    if urls_dict:
        save_urls_to_json(urls_dict)
        print(f"\nüìä Total: {count} images trouv√©es")
    else:
        print("\n‚ùå Aucune image 1080p n'a pu √™tre r√©cup√©r√©e")

if __name__ == "__main__":
    main()
